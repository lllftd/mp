#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆçˆ¬è™«è„šæœ¬ - ä¸€é”®å®Œæˆï¼šçˆ¬è™« â†’ AIè½¬è¿° â†’ æ°´å°æ¸…æ´— â†’ ä¸Šä¼ æ•°æ®åº“
"""
import datetime
import time
import random
import json
import csv
import os
import sys
import requests
from urllib.parse import quote
from DrissionPage._pages.chromium_page import ChromiumPage

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from spider_config import *
from ai_paraphrase import get_ai_paraphraser
from database import db
from config import Config
from batch_upload_tweets import insert_tweet, prepare_tweet_data
from username_generator import get_random_username

try:
    from PIL import Image
    import numpy as np
    import cv2
    IMAGE_PROCESSING_AVAILABLE = True
except ImportError:
    IMAGE_PROCESSING_AVAILABLE = False
    print("è­¦å‘Š: PIL/OpenCVæœªå®‰è£…ï¼Œæ°´å°æ¸…æ´—åŠŸèƒ½å°†ä½¿ç”¨AIæ–¹å¼")


class WatermarkRemover:
    """æ°´å°æ¸…æ´—å·¥å…·"""
    
    def __init__(self):
        self.use_ai = not IMAGE_PROCESSING_AVAILABLE
        self.ai_paraphraser = get_ai_paraphraser()
    
    def remove_watermark_ai(self, image_url: str) -> str:
        """ä½¿ç”¨AIæ¸…æ´—æ°´å°ï¼ˆå¦‚æœå›¾åƒå¤„ç†ä¸å¯ç”¨ï¼‰"""
        # å¯¹äºAIæ–¹å¼ï¼Œæˆ‘ä»¬ç›´æ¥è¿”å›åŸURLï¼Œå› ä¸ºAIä¸»è¦ç”¨äºå†…å®¹è½¬è¿°
        # å®é™…çš„æ°´å°æ¸…æ´—éœ€è¦ä¸“é—¨çš„å›¾åƒå¤„ç†AIæ¨¡å‹
        return image_url
    
    def remove_watermark_image(self, image_url: str, save_path: str = None) -> str:
        """ä½¿ç”¨å›¾åƒå¤„ç†æ¸…æ´—æ°´å°"""
        if not IMAGE_PROCESSING_AVAILABLE:
            return image_url
        
        try:
            # ä¸‹è½½å›¾ç‰‡
            response = requests.get(image_url, timeout=30)
            if response.status_code != 200:
                return image_url
            
            # è½¬æ¢ä¸ºPIL Image
            from io import BytesIO
            img = Image.open(BytesIO(response.content))
            img_array = np.array(img)
            
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            if len(img_array.shape) == 3:
                img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
            else:
                img_cv = img_array
            
            # ç®€å•çš„å»æ°´å°å¤„ç†ï¼šæ£€æµ‹å¹¶ç§»é™¤å¸¸è§çš„æ°´å°åŒºåŸŸ
            # æ–¹æ³•1: è¾¹ç¼˜æ£€æµ‹ + å½¢æ€å­¦æ“ä½œ
            gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # æ–¹æ³•2: æ£€æµ‹æ°´å°åŒºåŸŸï¼ˆé€šå¸¸åœ¨å³ä¸‹è§’æˆ–å·¦ä¸‹è§’ï¼‰
            h, w = img_cv.shape[:2]
            
            # æ£€æµ‹å³ä¸‹è§’åŒºåŸŸï¼ˆå¸¸è§æ°´å°ä½ç½®ï¼‰
            corner_region = img_cv[int(h*0.8):h, int(w*0.8):w]
            corner_gray = cv2.cvtColor(corner_region, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨inpaintingå¡«å……æ°´å°åŒºåŸŸ
            mask = np.zeros(corner_gray.shape[:2], np.uint8)
            # æ£€æµ‹ç™½è‰²/åŠé€æ˜åŒºåŸŸï¼ˆå¸¸è§æ°´å°ç‰¹å¾ï¼‰
            _, thresh = cv2.threshold(corner_gray, 200, 255, cv2.THRESH_BINARY)
            mask = thresh
            
            # æ‰©å±•maskåˆ°å…¨å›¾
            full_mask = np.zeros((h, w), np.uint8)
            full_mask[int(h*0.8):h, int(w*0.8):w] = mask
            
            # ä½¿ç”¨inpaintingå»é™¤æ°´å°
            result = cv2.inpaint(img_cv, full_mask, 3, cv2.INPAINT_TELEA)
            
            # è½¬æ¢å›PILå¹¶ä¿å­˜
            result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
            result_img = Image.fromarray(result_rgb)
            
            if save_path:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                result_img.save(save_path, quality=95)
                return save_path
            else:
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = f"temp_cleaned_{int(time.time())}.jpg"
                result_img.save(temp_path, quality=95)
                return temp_path
                
        except Exception as e:
            print(f"æ°´å°æ¸…æ´—å¤±è´¥: {e}")
            return image_url
    
    def download_image(self, image_url: str, save_path: str) -> str:
        """ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„"""
        try:
            response = requests.get(image_url, timeout=30)
            if response.status_code != 200:
                return ""
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ä¿å­˜å›¾ç‰‡
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            return save_path
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
            return ""
    
    def process_image(self, image_url: str, save_path: str = None) -> str:
        """å¤„ç†å›¾ç‰‡ï¼Œæ¸…æ´—æ°´å°"""
        if Config.REMOVE_WATERMARK:
            if IMAGE_PROCESSING_AVAILABLE:
                return self.remove_watermark_image(image_url, save_path)
            else:
                return self.remove_watermark_ai(image_url)
        elif save_path:
            return self.download_image(image_url, save_path)
        return image_url


class IntegratedSpider:
    """é›†æˆçˆ¬è™«ï¼šçˆ¬è™« + AIè½¬è¿° + æ°´å°æ¸…æ´— + ä¸Šä¼ """
    
    def __init__(self):
        self.page = ChromiumPage()
        self.setup_browser()
        self.request_count = 0
        self.last_request_time = 0
        self.ai_paraphraser = None
        self.watermark_remover = WatermarkRemover()
        
        # è‡ªåŠ¨å¯ç”¨AIè½¬è¿°
        try:
            self.ai_paraphraser = get_ai_paraphraser()
            if not self.ai_paraphraser.check_ollama_connection():
                raise Exception("OllamaæœåŠ¡æœªè¿è¡Œ")
            if not self.ai_paraphraser.check_model_exists():
                raise Exception(f"æ¨¡å‹ {Config.LLM_MODEL} æœªä¸‹è½½")
            print("âœ… AIè½¬è¿°åŠŸèƒ½å·²å¯ç”¨")
        except Exception as e:
            print(f"âŒ AIè½¬è¿°åˆå§‹åŒ–å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ï¼š")
            print("1. OllamaæœåŠ¡å·²è¿è¡Œ")
            print(f"2. æ¨¡å‹ {Config.LLM_MODEL} å·²ä¸‹è½½")
            print("è¿è¡Œ: python setup_ollama.py")
            raise
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨å‚æ•°"""
        user_agent = random.choice(USER_AGENTS)
        headers = DEFAULT_HEADERS.copy()
        headers['User-Agent'] = user_agent
        self.page.set.headers(headers)
        self.page.set.window.size(WINDOW_WIDTH, WINDOW_HEIGHT)
    
    def enforce_rate_limit(self):
        """å¼ºåˆ¶é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < MIN_REQUEST_INTERVAL:
            sleep_time = MIN_REQUEST_INTERVAL - time_since_last
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
        
        if self.request_count % EXTRA_DELAY_INTERVAL == 0:
            extra_delay = random.uniform(EXTRA_DELAY_MIN, EXTRA_DELAY_MAX)
            time.sleep(extra_delay)
    
    def random_delay(self, min_delay=None, max_delay=None):
        """éšæœºå»¶è¿Ÿ"""
        if min_delay is None:
            min_delay = DELAY_MIN
        if max_delay is None:
            max_delay = DELAY_MAX
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
    
    def human_like_scroll(self):
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨"""
        scroll_steps = random.randint(SCROLL_STEPS_MIN, SCROLL_STEPS_MAX)
        for i in range(scroll_steps):
            scroll_distance = random.randint(SCROLL_DISTANCE_MIN, SCROLL_DISTANCE_MAX)
            self.page.run_js(f"window.scrollBy(0, {scroll_distance})")
            time.sleep(random.uniform(SCROLL_INTERVAL_MIN, SCROLL_INTERVAL_MAX))
    
    def check_for_blocking(self):
        """æ£€æŸ¥æ˜¯å¦è¢«é˜»æ­¢"""
        try:
            page_text = self.page.html.lower()
            for indicator in BLOCKING_INDICATORS:
                if indicator in page_text:
                    return True
            return False
        except:
            return False
    
    def handle_blocking(self):
        """å¤„ç†è¢«é˜»æ­¢çš„æƒ…å†µ"""
        wait_time = random.uniform(BLOCKING_WAIT_MIN, BLOCKING_WAIT_MAX)
        time.sleep(wait_time)
        try:
            self.page.refresh()
            self.page.wait.doc_loaded()
            time.sleep(random.uniform(3, 5))
        except:
            pass
    
    def get_note_detail(self, note_id, xsec_token, retry_count=0):
        """è·å–ç¬”è®°è¯¦æƒ…"""
        try:
            self.enforce_rate_limit()
            infourl = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source=pc_search&source=web_explore_feed"
            print(f"æ­£åœ¨è·å–è¯¦æƒ…ï¼š{note_id}")
            
            self.random_delay(3, 6)
            self.page.get(infourl)
            self.page.wait.doc_loaded()
            
            if self.check_for_blocking():
                self.handle_blocking()
                if retry_count < MAX_RETRIES:
                    return self.get_note_detail(note_id, xsec_token, retry_count + 1)
                else:
                    return "", "", ""
            
            self.human_like_scroll()
            
            # è·å–å›¾ç‰‡é“¾æ¥
            img_urls = []
            try:
                swiper_elements = self.page.eles('.swiper-wrapper')
                if swiper_elements:
                    images = swiper_elements[0].eles("tag:img")
                    for img in images:
                        try:
                            imgurl = img.attr("src")
                            if imgurl and imgurl not in img_urls:
                                img_urls.append(imgurl)
                        except:
                            pass
            except Exception as e:
                print(f"è·å–å›¾ç‰‡å¤±è´¥: {e}")
            
            # è·å–æ ‡é¢˜å’Œæè¿°
            title = ""
            desc = ""
            try:
                title_ele = self.page.ele("#detail-title")
                if title_ele:
                    title = title_ele.text.strip()
                    
                desc_ele = self.page.ele("#detail-desc")
                if desc_ele:
                    desc = desc_ele.text.strip()
            except Exception as e:
                print(f"è·å–æ ‡é¢˜æˆ–æè¿°å¤±è´¥: {e}")
                
            return title, desc, ",".join(img_urls)
            
        except Exception as e:
            print(f"è·å–è¯¦æƒ…å¤±è´¥: {e}")
            if retry_count < MAX_RETRIES:
                self.random_delay(8, 15)
                return self.get_note_detail(note_id, xsec_token, retry_count + 1)
            else:
                return "", "", ""
    
    def search_notes(self, keyword, pages):
        """æœç´¢ç¬”è®°"""
        try:
            self.page.listen.start("https://edith.xiaohongshu.com/api/sns/web/v1/search/notes")
            encoded_keyword = quote(keyword)
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_explore_feed"
            
            print(f"æ­£åœ¨è®¿é—®æœç´¢é¡µé¢: {search_url}")
            self.page.get(search_url)
            self.page.wait.doc_loaded()
            self.random_delay(5, 8)
            
            responses = []
            
            for page_num in range(pages):
                try:
                    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page_num + 1} é¡µ")
                    
                    if self.check_for_blocking():
                        self.handle_blocking()
                        continue
                    
                    self.human_like_scroll()
                    
                    try:
                        packet = self.page.listen.wait(timeout=REQUEST_TIMEOUT)
                        if packet and packet.response:
                            response_body = packet.response.body
                            if response_body:
                                responses.append(response_body)
                                print(f"æˆåŠŸæ•è·ç¬¬ {page_num + 1} é¡µæ•°æ®")
                    except Exception as e:
                        print(f"ç¬¬ {page_num + 1} é¡µæ•è·å¤±è´¥: {e}")
                    
                    if page_num < pages - 1:
                        page_delay = random.uniform(PAGE_DELAY_MIN, PAGE_DELAY_MAX)
                        time.sleep(page_delay)
                        
                except KeyboardInterrupt:
                    print(f"\nâš ï¸  åœ¨ç¬¬ {page_num + 1} é¡µçˆ¬å–æ—¶è¢«ä¸­æ–­")
                    print(f"å·²è·å– {len(responses)} é¡µæ•°æ®")
                    raise
            
            return responses
            
        except Exception as e:
            print(f"æœç´¢ç¬”è®°å¤±è´¥: {e}")
            return []
    
    def process_and_upload(self, responses, keyword):
        """å¤„ç†æ•°æ®å¹¶ä¸Šä¼ """
        total_notes = 0
        processed_count = 0
        ai_paraphrased_count = 0
        upload_success_count = 0
        upload_fail_count = 0
        
        nowt = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        
        # åˆ›å»ºä¿å­˜ç›®å½•ç»“æ„: saved/æ—¶é—´æˆ³/å›¾ç‰‡ã€åŸæ–‡ã€è½¬è¿°
        base_dir = os.path.join("saved", nowt)
        images_dir = os.path.join(base_dir, "å›¾ç‰‡")
        original_dir = os.path.join(base_dir, "åŸæ–‡")
        paraphrased_dir = os.path.join(base_dir, "è½¬è¿°")
        
        os.makedirs(images_dir, exist_ok=True)
        os.makedirs(original_dir, exist_ok=True)
        os.makedirs(paraphrased_dir, exist_ok=True)
        
        print(f"ä¿å­˜ç›®å½•: {base_dir}")
        
        csv_filename = os.path.join(base_dir, f"{keyword}_{nowt}.csv")
        
        with open(csv_filename, 'w', encoding='utf-8-sig', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["é¤å…åç§°", "åŸå§‹æè¿°", "å›¾ç‰‡é“¾æ¥", "ç¬”è®°ID", "è½¬è¿°æ ‡é¢˜", "è½¬è¿°æè¿°", "åœ°å€", "æ¸…æ´—åå›¾ç‰‡"])
            
            for response in responses:
                try:
                    if isinstance(response, str):
                        response_data = json.loads(response)
                    else:
                        response_data = response
                    
                    if 'data' in response_data and 'items' in response_data['data']:
                        notes = response_data['data']['items']
                        total_notes += len(notes)
                        
                        for note in notes:
                            try:
                                note_id = note.get("id")
                                xsec_token = note.get("xsec_token")
                                
                                if note_id and xsec_token:
                                    title, desc, img = self.get_note_detail(note_id, xsec_token)
                                    if title:
                                        # ä¿å­˜åŸæ–‡
                                        original_filename = os.path.join(original_dir, f"{processed_count:04d}_{note_id}.txt")
                                        with open(original_filename, 'w', encoding='utf-8') as f:
                                            f.write(f"æ ‡é¢˜: {title}\n\n")
                                            f.write(f"æè¿°: {desc}\n")
                                        print(f"\nğŸ“ æ­£åœ¨å¤„ç†ç¬”è®°: {title[:50]}...")
                                        
                                        # æå–é¤å…ä¿¡æ¯
                                        print(f"ğŸ” æ­£åœ¨æå–é¤å…ä¿¡æ¯...")
                                        restaurants = self.ai_paraphraser.extract_restaurants(title, desc)
                                        
                                        if not restaurants:
                                            # å¦‚æœæ²¡æœ‰æå–åˆ°é¤å…ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼å¤„ç†ï¼ˆä½œä¸ºå•ä¸ªæ¡ç›®ï¼‰
                                            print(f"âš ï¸  æœªæå–åˆ°é¤å…ä¿¡æ¯ï¼ŒæŒ‰åŸç¬”è®°å¤„ç†")
                                            restaurants = [{
                                                'name': title,
                                                'address': '',
                                                'price_range': '',
                                                'description': desc
                                            }]
                                        
                                        print(f"âœ… æå–åˆ° {len(restaurants)} ä¸ªé¤å…")
                                        
                                        # ä¸‹è½½å¹¶æ¸…æ´—å›¾ç‰‡ï¼ˆæ‰€æœ‰é¤å…å…±äº«ï¼‰
                                        saved_image_paths = []
                                        if img:
                                            img_list = img.split(',')
                                            for idx, img_url in enumerate(img_list):
                                                if img_url.strip():
                                                    print(f"æ­£åœ¨å¤„ç†å›¾ç‰‡ {idx+1}/{len(img_list)}: {img_url[:50]}...")
                                                    
                                                    # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
                                                    img_ext = os.path.splitext(img_url.split('?')[0])[1] or '.jpg'
                                                    img_filename = f"{processed_count:04d}_{note_id}_{idx+1}{img_ext}"
                                                    img_path = os.path.join(images_dir, img_filename)
                                                    
                                                    # æ¸…æ´—æ°´å°å¹¶ä¿å­˜
                                                    if Config.REMOVE_WATERMARK and IMAGE_PROCESSING_AVAILABLE:
                                                        cleaned_img = self.watermark_remover.remove_watermark_image(img_url.strip(), img_path)
                                                        if cleaned_img and os.path.exists(cleaned_img):
                                                            saved_image_paths.append(cleaned_img)
                                                            print(f"âœ… æ°´å°æ¸…æ´—å®Œæˆ: {img_filename}")
                                                    else:
                                                        # ç›´æ¥ä¸‹è½½åŸå›¾
                                                        saved_path = self.watermark_remover.download_image(img_url.strip(), img_path)
                                                        if saved_path:
                                                            saved_image_paths.append(saved_path)
                                                            print(f"âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆ: {img_filename}")
                                        
                                        cleaned_img_str = ",".join(saved_image_paths) if saved_image_paths else img
                                        
                                        # ä¸ºæ¯ä¸ªé¤å…åˆ†åˆ«è½¬è¿°å’Œä¸Šä¼ 
                                        for restaurant_idx, restaurant in enumerate(restaurants):
                                            restaurant_name = restaurant.get('name', 'æœªçŸ¥é¤å…')
                                            print(f"\nğŸ´ æ­£åœ¨å¤„ç†é¤å… {restaurant_idx + 1}/{len(restaurants)}: {restaurant_name}")
                                            
                                            # å¯¹é¤å…è¿›è¡Œè½¬è¿°
                                            paraphrased_title, paraphrased_desc, type_cid = self.ai_paraphraser.paraphrase_restaurant(restaurant, title)
                                            
                                            if not paraphrased_title:
                                                print(f"âš ï¸  é¤å…è½¬è¿°å¤±è´¥ï¼Œè·³è¿‡")
                                                continue
                                            
                                            if not type_cid:
                                                type_cid = Config.DEFAULT_TYPE_CID if Config.DEFAULT_TYPE_CID else "10"
                                                print(f"âš ï¸  ä½¿ç”¨é»˜è®¤å­ç±»å‹ID: {type_cid}")
                                            else:
                                                print(f"âœ… AIåˆ†ç±»å®Œæˆ: å­ç±»å‹ID={type_cid}")
                                            
                                            # ä¿å­˜è½¬è¿°å†…å®¹ï¼ˆæ¯ä¸ªé¤å…å•ç‹¬ä¿å­˜ï¼‰
                                            restaurant_safe_name = restaurant_name.replace('/', '_').replace('\\', '_')[:50]
                                            paraphrased_filename = os.path.join(paraphrased_dir, f"{processed_count:04d}_{note_id}_{restaurant_idx}_{restaurant_safe_name}.txt")
                                            with open(paraphrased_filename, 'w', encoding='utf-8') as f:
                                                f.write(f"é¤å…åç§°: {restaurant_name}\n\n")
                                                f.write(f"æ ‡é¢˜: {paraphrased_title}\n\n")
                                                f.write(f"æè¿°: {paraphrased_desc}\n")
                                                if restaurant.get('address'):
                                                    f.write(f"\nåœ°å€: {restaurant.get('address')}\n")
                                                if restaurant.get('price_range'):
                                                    f.write(f"\näººå‡: {restaurant.get('price_range')}\n")
                                            print(f"âœ… å·²ä¿å­˜è½¬è¿°: {paraphrased_filename}")
                                            
                                            # å†™å…¥CSVï¼ˆæ¯ä¸ªé¤å…ä¸€è¡Œï¼‰
                                            writer.writerow([
                                                restaurant_name,  # åŸå§‹æ ‡é¢˜ï¼ˆé¤å…åï¼‰
                                                restaurant.get('description', desc),  # åŸå§‹æè¿°
                                                img,  # åŸå§‹å›¾ç‰‡é“¾æ¥
                                                f"{note_id}_{restaurant_idx}",  # ç¬”è®°ID_é¤å…ç´¢å¼•
                                                paraphrased_title,  # è½¬è¿°æ ‡é¢˜
                                                paraphrased_desc,  # è½¬è¿°æè¿°
                                                restaurant.get('address', ''),  # åœ°å€ï¼ˆä½œä¸ºå†…å®¹ç±»å‹å­—æ®µï¼‰
                                                cleaned_img_str  # æ¸…æ´—åå›¾ç‰‡
                                            ])
                                            
                                            # å‡†å¤‡æ•°æ®åº“æ•°æ®ï¼ˆæ¯ä¸ªé¤å…ä¸€æ¡è®°å½•ï¼‰
                                            tweet = {
                                                'tweets_title': paraphrased_title,
                                                'tweets_content': paraphrased_desc,
                                                'tweets_describe': paraphrased_desc[:200] if len(paraphrased_desc) > 200 else paraphrased_desc,
                                                'tweets_img': json.dumps(saved_image_paths) if saved_image_paths else json.dumps(img.split(',') if img else []),
                                                'tweets_type_pid': Config.DEFAULT_TYPE_PID,
                                                'tweets_type_cid': type_cid,  # ä½¿ç”¨AIè¿”å›çš„å­ç±»å‹ID
                                                'tweets_user': get_random_username(),  # éšæœºç”Ÿæˆç”¨æˆ·å
                                            }
                                            
                                            # ç«‹å³ä¸Šä¼ åˆ°æ•°æ®åº“
                                            try:
                                                prepared_tweet = prepare_tweet_data(tweet)
                                                tweet_id = insert_tweet(prepared_tweet)
                                                if tweet_id:
                                                    upload_success_count += 1
                                                    print(f"âœ… ä¸Šä¼ è‡³æ•°æ®åº“å®Œæˆ (ID: {tweet_id}) - {restaurant_name}")
                                                else:
                                                    upload_fail_count += 1
                                                    print(f"âŒ ä¸Šä¼ è‡³æ•°æ®åº“å¤±è´¥: è¿”å›IDä¸ºç©º - {restaurant_name}")
                                            except Exception as e:
                                                upload_fail_count += 1
                                                print(f"âŒ ä¸Šä¼ è‡³æ•°æ®åº“å¤±è´¥: {e} - {restaurant_name}")
                                            
                                            processed_count += 1
                                            ai_paraphrased_count += 1
                                            
                                            if processed_count % BATCH_SIZE == 0:
                                                file.flush()
                                                print(f"å·²å¤„ç† {processed_count} æ¡æ•°æ®")
                                                self.random_delay(3, 6)
                            except KeyboardInterrupt:
                                print(f"\nâš ï¸  åœ¨å¤„ç†ç¬¬ {processed_count + 1} æ¡æ•°æ®æ—¶è¢«ä¸­æ–­")
                                print(f"å·²å¤„ç† {processed_count} æ¡æ•°æ®")
                                raise
                            except Exception as e:
                                print(f"å¤„ç†å•æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€æ¡
                except KeyboardInterrupt:
                    print(f"\nâš ï¸  åœ¨å¤„ç†å“åº”æ—¶è¢«ä¸­æ–­")
                    print(f"å·²å¤„ç† {processed_count} æ¡æ•°æ®")
                    raise
                except Exception as e:
                    print(f"å¤„ç†å“åº”æ—¶å‡ºç°é”™è¯¯: {e}")
        
        print(f"\næ€»å…±å¤„ç†äº† {total_notes} æ¡ç¬”è®°ï¼ŒæˆåŠŸä¿å­˜ {processed_count} æ¡é¤å…æ•°æ®")
        print(f"ä¿å­˜ä½ç½®: {base_dir}")
        print(f"AIè½¬è¿°æˆåŠŸ: {ai_paraphrased_count} æ¡")
        print(f"æ•°æ®åº“ä¸Šä¼ : æˆåŠŸ {upload_success_count} æ¡, å¤±è´¥ {upload_fail_count} æ¡")
        
        return csv_filename
    
    def login(self):
        """æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦"""
        self.page.get('https://www.xiaohongshu.com')
        print('è¯·æ‰«ç ç™»å½•å°çº¢ä¹¦')
        print('ç™»å½•å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...')
        input()
        time.sleep(3)
    
    def run(self, keyword, pages):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        try:
            self.login()
            
            print(f"\nå¼€å§‹æŠ“å–å…³é”®è¯ï¼š{keyword}")
            print(f"é¢„è®¡æŠ“å– {pages} é¡µæ•°æ®")
            print("=" * 60)
            print("æç¤º: æŒ‰ Ctrl+C å¯ä»¥éšæ—¶ç»ˆæ­¢ç¨‹åº")
            
            responses = self.search_notes(keyword, pages)
            if responses:
                filename = self.process_and_upload(responses, keyword)
                print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°ï¼š{filename}")
            else:
                print("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸  æ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
            print("æ­£åœ¨ä¿å­˜å·²å¤„ç†çš„æ•°æ®...")
            print("æ­£åœ¨å…³é—­æµè§ˆå™¨...")
            raise  # é‡æ–°æŠ›å‡ºï¼Œè®©å¤–å±‚ä¹Ÿèƒ½æ•è·
        except Exception as e:
            print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        finally:
            try:
                self.page.close()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            except:
                pass


def main():
    """ä¸»å‡½æ•° - ä¸€é”®è¿è¡Œ"""
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python crawler.py <å…³é”®è¯> <é¡µæ•°>")
        print("ç¤ºä¾‹: python crawler.py æ·±åœ³ç¾é£Ÿ 5")
        sys.exit(1)
    
    keyword = sys.argv[1]
    try:
        pages = int(sys.argv[2])
    except ValueError:
        print("é”™è¯¯: é¡µæ•°å¿…é¡»æ˜¯æ•°å­—")
        sys.exit(1)
    
    print("=" * 60)
    print("é›†æˆçˆ¬è™«ç³»ç»Ÿ")
    print("åŠŸèƒ½: çˆ¬è™« â†’ AIè½¬è¿° â†’ æ°´å°æ¸…æ´— â†’ è‡ªåŠ¨ä¸Šä¼ ")
    print(f"æ¨¡å‹: {Config.LLM_MODEL}")
    print("=" * 60)
    print()
    
    spider = None
    try:
        spider = IntegratedSpider()
        spider.run(keyword, pages)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        print("æ­£åœ¨æ¸…ç†èµ„æº...")
        if spider:
            try:
                spider.page.close()
            except:
                pass
        print("âœ… ç¨‹åºå·²å®‰å…¨é€€å‡º")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        if spider:
            try:
                spider.page.close()
            except:
                pass
        sys.exit(1)


if __name__ == '__main__':
    main()

